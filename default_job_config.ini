[logging]
level = INFO

[job]
target_timezone = America/Chicago

[pull_job]
raw_data_path = /opt/daily_pipeline_car_crash/data/crash_data
# name vs id
datasets =  {"crashes": {"id": "85ca-t3if", "time_column" : "crash_date"},  "crashes_people": {"id": "u6pd-qa9d", "time_column" : "crash_date"}, "crashes_vehicles": {"id": "68nd-jvt3", "time_column" : "crash_date"}}
timeout_sec = 10
batch_size = 2000
sleep_time_millis = 100
domain = data.cityofchicago.org

[iceberg]
catalog_name = local
catalog_type = hadoop
db = db
wh_path = /opt/daily_pipeline_car_crash/data/iceberg_crashes
key = ${SECRET_KEY}
#wh_path = /Users/vishnuch/work/gitcode/bytespireio/streamlana/data/iceberg_crashes

[cubes]
json_path = /opt/daily_pipeline_car_crash/cubes.json
output_path = /tmp/cubes

[spark]
spark.local.dir	 = /tmp/spark-temp
spark.master	 = local
spark.driver.memory = 2g
spark.jars.packages = org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.1,io.github.jaihind213:spark-set-udaf:spark3.5.2-scala2.13-1.0.1-jdk11
spark.sql.extensions = org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.session.timeZone = GMT
spark.sql.legacy.parquet.nanosAsLong = true
spark.sql.parquet.timestampNTZ.enabled = true
#spark.sql.shuffle.partitions = 1
#spark.sql.parquet.compression.codec	= brotli
