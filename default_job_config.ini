[logging]
log_level = INFO

[job]
target_timezone = America/Chicago
blob_storage_endpoint_url = https://sgp1.digitaloceanspaces.com

[pull_job]
#raw_data_path = /opt/daily_pipeline_car_crash/data/crash_data
raw_data_path = ${RAW_DATA_PATH:/tmp/crash_data_raw}
# name vs id
datasets =  {"crashes": {"id": "85ca-t3if", "time_column" : "crash_date"},  "crashes_people": {"id": "u6pd-qa9d", "time_column" : "crash_date"}, "crashes_vehicles": {"id": "68nd-jvt3", "time_column" : "crash_date"}}
timeout_sec = 10
batch_size = 2000
sleep_time_millis = 100
domain = data.cityofchicago.org
socrata_token= ${SOCRATA_TOKEN:abc}

[iceberg]
catalog_name = local
catalog_type = hadoop
db = db
wh_path = ${ICEBERG_PATH:/tmp/crash_data_iceberg}

[cubes]
json_path = /opt/daily_pipeline_car_crash/cubes.json
output_path = /tmp/cubes

[spark]
spark.local.dir	 = /tmp/spark-temp
#spark.master	 = local
#spark.jars.packages = org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.8.1,io.github.jaihind213:spark-set-udaf:spark3.5.2-scala2.13-1.0.1-jdk11,org.apache.hadoop:hadoop-aws:3.4.1
#hadoop-aws use the same version of hadoop jars in SPARK_HOME/jars
#set JAVA_HOME/SPARK_HOME in the image
spark.jars.packages = org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.9.1,io.github.jaihind213:spark-set-udaf:spark3.5.2-scala2.13-1.0.1-jdk11,org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-client:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.787
#spark.jars.packages = org.apache.hadoop:hadoop-common:3.3.6,org.apache.hadoop:hadoop-aws:3.4.1,org.apache.iceberg:iceberg-spark-runtime-3.5_2.13:1.9.1,io.github.jaihind213:spark-set-udaf:spark3.5.2-scala2.13-1.0.1-jdk11
#spark jars always relative to project path
#spark.jars = jars/hadoop-aws-3.4.1.jar
#spark.driver.extraClassPath = /Users/vishnuch/work/utils/spark-3.5.2-bin-hadoop3-scala2.13/jars
#spark.executor.extraClassPath = /Users/vishnuch/work/utils/spark-3.5.2-bin-hadoop3-scala2.13/jars
# add the spark packages jars to extraClassPath in the image
spark.driver.extraClassPath = /opt/spark_jars/
spark.executor.extraClassPath = /opt/spark_jars/
spark.sql.extensions = org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.session.timeZone = GMT
spark.sql.legacy.parquet.nanosAsLong = true
spark.sql.parquet.timestampNTZ.enabled = true
spark.hadoop.fs.s3a.access.key = ${S3_ACCESS_KEY:abc}
spark.hadoop.fs.s3a.secret.key = ${S3_SECRET_KEY:foo}
spark.hadoop.fs.s3a.endpoint= ${S3_ENDPOINT:sgp1.digitaloceanspaces.com}
#spark.hadoop.fs.s3a.impl= "org.apache.hadoop.fs.s3b.S3AFileSystem"
#spark.sql.shuffle.partitions = 1
#spark.sql.parquet.compression.codec	= brotli
