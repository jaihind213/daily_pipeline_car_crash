name: Setup & Run Daily Chicago Car Crash Pipeline

on:
#  push:
#    branches:
#      - dev
  workflow_dispatch:
    inputs:
      should_tear_down:
        description: 'teardown environment ?'
        required: true
        default: 'true'
      stack_name:
        description: 'stack name ?'
        required: true
        default: 'prod'
  schedule:
    - cron: '0 2 * * *'  # Runs daily at 2:00 AM GMT

env:
  K8S_BOOTUP_TIME_SEC: ${{ vars.K8S_BOOTUP_TIME_SEC }}
  DAG_RUN_TIME_SEC: ${{ vars.DAG_RUN_TIME_SEC }}
  DAG_NAME: chicago_car_crash_pipeline
  #PULUMI_STACK_NAME: prod
jobs:
  setup_infra:
    runs-on: ubuntu-latest
    outputs:
      k8s_cluster_id: ${{ steps.deploy_pulumi_stack.outputs.cluster_id }}
    environment: cicd
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: v3.14.0

      - name: Install doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DO_TOKEN }}

      - name: üêçSet up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd infra-as-code
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Pulumi CLI
        uses: pulumi/actions@v4
        with:
          pulumi-version: latest

      - name: Login to Pulumi
        run: pulumi login
        env:
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}
      - name: Create K8s cluster id
        run: |
          export PULUMI_STACK_NAME=${{ github.event.inputs.stack_name }}
          if [ -z "$PULUMI_STACK_NAME" ]; then
            echo "PULUMI_STACK_NAME is not set, using default 'prod'"
            export PULUMI_STACK_NAME="prod"
          fi
          echo "PULUMI_STACK_NAME=$PULUMI_STACK_NAME" >> $GITHUB_ENV
          echo ${{ env.PULUMI_STACK_NAME }}
          #k8s-1-33-1-do-2-sgp1-1753321136291
          #pool-k81x1jrz8
          prefix="k8s-1-33-1-do-2-sgp1"
          random_num=$(( RANDOM % 9 + 1 ))$(printf "%012d" $RANDOM$RANDOM | cut -c1-12)   
          rand_str=$(echo $(tr -dc 'a-z' </dev/urandom | head -c1)$(tr -dc 'a-z0-9' </dev/urandom | head -c8))
          export KUBERNETES_CLUSTER_ID="$prefix-$random_num"
          export KUBERNETES_POOL_ID="pool-$rand_str"
          echo "will use k8s_cluster_id_to_use=$KUBERNETES_CLUSTER_ID"
          echo "will use pool_id=$KUBERNETES_POOL_ID"
          echo "KUBERNETES_CLUSTER_ID=$KUBERNETES_CLUSTER_ID" >> $GITHUB_ENV
          echo "KUBERNETES_POOL_ID=$KUBERNETES_POOL_ID" >> $GITHUB_ENV
      - name: üèóÔ∏èSet up Pulumi stack
        id: setup_up_pulumi_stack
        run: |
          export PULUMI_STACK_NAME=${{ env.PULUMI_STACK_NAME }}
          export KUBERNETES_CLUSTER_ID=${{ env.KUBERNETES_CLUSTER_ID }}
          export KUBERNETES_POOL_ID=${{ env.KUBERNETES_POOL_ID }}
          cd infra-as-code
          #echo "PULUMI_STACK_NAME=$PULUMI_STACK_NAME" >> $GITHUB_ENV
          pulumi stack select $PULUMI_STACK_NAME || pulumi stack init $PULUMI_STACK_NAME
          pulumi config set digitalocean:token ${{ secrets.DO_TOKEN }} --secret
          pulumi refresh --yes
          OUTPUT=$(pulumi stack output k8s_cluster_id || echo "")
          if [ -n "$OUTPUT" ]; then
            echo "K8S_CLUSTER_ID=$OUTPUT" >> $GITHUB_ENV
            echo "‚úÖ Exported K8S_CLUSTER_ID"
            #export K8S_CLUSTER_ID=$OUTPUT
            #echo "cluster_id=$K8S_CLUSTER_ID" >> $GITHUB_OUTPUT
          fi
      - name: üööDeploy with Pulumi
        id: deploy_pulumi_stack
        run: |
          export PULUMI_STACK_NAME=${{ env.PULUMI_STACK_NAME }}
          export KUBERNETES_CLUSTER_ID=${{ env.KUBERNETES_CLUSTER_ID }}
          export KUBERNETES_POOL_ID=${{ env.KUBERNETES_POOL_ID }}
          cd infra-as-code
          pulumi up --yes
          #pulumi stack select prod
          #pulumi destroy --remove --yes
          K8S_CLUSTER_ID=$(pulumi stack output k8s_cluster_id || echo "")
          echo "K8S_CLUSTER_ID=$K8S_CLUSTER_ID" >> $GITHUB_ENV
          echo "cluster_id=$K8S_CLUSTER_ID" >> $GITHUB_OUTPUT
      - name: üíæSave DigitalOcean kubeconfig
        run: |
          cd infra-as-code
          doctl kubernetes cluster kubeconfig save ${{ env.K8S_CLUSTER_ID }}
      - name: ‚ö†Ô∏è Check if k8s nodes ready
        run: |
          #for safety adding this sleep
          sleep ${{ env.K8S_BOOTUP_TIME_SEC }}
          export NOT_READY_NODES=$(kubectl get nodes --no-headers | awk '$2 != "Ready"')
          if [ -n "$NOT_READY_NODES" ]; then
            echo "‚ö†Ô∏è Some nodes are NotReady: lets wait"
            echo "$NOT_READY_NODES"
            sleep ${{ env.K8S_BOOTUP_TIME_SEC }}
            export NOT_READY_NODES=$(kubectl get nodes --no-headers | awk '$2 != "Ready"')
            if [ -n "$NOT_READY_NODES" ]; then
                echo "‚ùå Some nodes are still NotReady after waiting:"
                echo "$NOT_READY_NODES"
                exit 1
            else
              echo "‚úÖ All nodes are Ready."
            fi
          else
            echo "‚úÖ All nodes are Ready."  
          fi
  setup_spark_airflow_operator:
    runs-on: ubuntu-latest
    needs: [ setup_infra ]
    environment: cicd
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: v3.14.0

      - name: ü¶≠Install kubeseal
        uses: digitalservicebund/setup-kubeseal@v1.0.0

      - name: Install envsubst
        run: sudo apt-get update && sudo apt-get install -y gettext

      - name: Install doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DO_TOKEN }}

      - name: üêçSet up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd infra-as-code
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Pulumi CLI
        uses: pulumi/actions@v4
        with:
          pulumi-version: latest

      - name: Login to Pulumi
        run: pulumi login
        env:
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}

      - name: üíæSave DigitalOcean kubeconfig
        run: |
          cd infra-as-code
          doctl kubernetes cluster kubeconfig save ${{ needs.setup_infra.outputs.k8s_cluster_id }}
      - name: Add helm repos
        run: |
          helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets
          helm repo update
          helm repo add apache-airflow https://airflow.apache.org
          helm repo update
          #helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator # gives 404, was working.
          helm uninstall spark-operator -n spark-operator  || echo "it ok to fail"
          helm uninstall airflow -n airflow  || echo "it ok to fail"
          helm repo add --force-update spark-operator https://kubeflow.github.io/spark-operator
          helm repo update
          helm repo list
      - name: ‚ú®Spark Setup
        run: |
          kubectl create namespace airflow || echo "airflow namespace exists"
          kubectl delete namespace spark-operator || echo "it is ok to fail"
          kubectl create namespace spark-operator || echo "spark-operator namespace exists"
          #git clone https://github.com/GoogleCloudPlatform/spark-on-k8s-operator.git
          #cd spark-on-k8s-operator/charts/spark-operator-chart
          #helm install spark-operator spark-operator/spark-operator  --set webhook.enable=true --set webhook.port=443 --namespace spark-operator --create-namespace
          #helm install spark-operator .  --set webhook.enable=true --set webhook.port=443 --namespace spark-operator --create-namespace
          #helm install spark-operator spark-operator/spark-operator --set sparkJobNamespace=*  --set webhook.enable=true --set webhook.enableCertManager=false --set webhook.servicePort=443 --set webhook.port=8443 --namespace spark-operator --create-namespace
          #helm install spark-operator spark-operator/spark-operator  --set serviceAccounts.spark.name=spark-operator-controller --set sparkjobNamespaces={airflow}  --set webhook.enable=true --set webhook.enableCertManager=false --set webhook.servicePort=443 --set webhook.port=8443 --namespace spark-operator
          helm install spark-operator spark-operator/spark-operator  --set serviceAccounts.spark.name=spark-operator-controller --set spark.jobNamespaces={airflow}  --set webhook.enable=true --set webhook.enableCertManager=false --set webhook.servicePort=443 --set webhook.port=8443 --namespace spark-operator
          
          sleep 180
          helm repo list
          cd infra-as-code/k8s
          kubectl apply -n airflow -f spark-rbac.yaml
          kubectl apply -n airflow -f spark-operator-cluster-rbac.yaml
          #check any errors
          kubectl logs -n spark-operator -l app.kubernetes.io/name=spark-operator |grep -vi err
          #kubectl apply -f spark-operator-role-binding.yaml
          #kubectl apply -f spark-operator-role.yaml
      - name: üåÄ‡ºÑ Airflow secret üîê
        run: |
          kubectl create namespace sealed-secrets  || echo "sealed-secrets namespace exists"
          helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets || echo "sealed secrets repo added"
          helm repo update
          helm upgrade --install sealed-secrets-controller sealed-secrets/sealed-secrets --namespace sealed-secrets
          sleep 90
          kubectl get pods -A | grep sealed-secrets || echo "tried to get pods"
          kubectl create secret generic git-credentials --namespace airflow  --dry-run=client --from-literal=GIT_SYNC_USERNAME='${{ secrets.GIT_SYNC_USERNAME }}' --from-literal=GITSYNC_USERNAME='${{ secrets.GIT_SYNC_USERNAME }}' --from-literal=GIT_SYNC_PASSWORD='${{ secrets.GIT_SYNC_PASSWORD }}' --from-literal=GITSYNC_PASSWORD='${{ secrets.GIT_SYNC_PASSWORD }}' --from-literal=password=LOL -o json | kubeseal --namespace airflow --controller-namespace=sealed-secrets --scope=strict --format=yaml > my-secret-sealed.yaml
          kubectl apply -f my-secret-sealed.yaml -n airflow
          export SOCRATA_TOKEN=${{ secrets.SOCRATA_APP_TOKEN }}
          export S3_ACCESS_KEY=${{ secrets.S3_ACCESS_KEY }}
          export S3_SECRET_KEY=${{ secrets.S3_SECRET_KEY }}
          export ICEBERG_PATH=${{ secrets.ICEBERG_PATH }}
          export RAW_DATA_PATH=${{ secrets.RAW_DATA_PATH }}
          cd infra-as-code/k8s
          envsubst < dag_secrets_template.yaml | kubectl create  --namespace airflow -f - --dry-run=client -o json | kubeseal --namespace airflow --controller-namespace=sealed-secrets --scope=strict --format=yaml > dag-sealed-secret.yaml
          kubectl apply -f dag-sealed-secret.yaml -n airflow
      - name: üåÄ‡ºÑ Airflow setup
        run: |
          cd infra-as-code/airflow
          export PGKEY=${{ secrets.PGKEY }}
          export AF_FN=${{ secrets.AF_FN }}
          export AF_LN=${{ secrets.AF_LN }}
          export AF_EMAIL=${{ secrets.AF_EMAIL }}
          export AF_USER=${{ secrets.AF_USER }}
          export AF_KEY=${{ secrets.AF_KEY }}

          envsubst < airflow_values_template.yaml > airflow_values.yaml
          helm repo add apache-airflow https://airflow.apache.org || echo "airflow repo added"
          helm repo update
          helm upgrade --install airflow apache-airflow/airflow -n airflow --create-namespace -f airflow_values.yaml
          sleep 180
      - name: üì£ Apply Configs
        run: |
          cd infra-as-code/k8s/app_configs
          kubectl apply -n airflow -f common_config_map.yaml
          IMAGE=$(kubectl get configmap image-config-map -n airflow -o jsonpath="{.data.image}")
          for NODE in $(kubectl get nodes -o name); do
            echo "Pulling image $IMAGE on $NODE"
            
            kubectl debug "$NODE" \
              --image="$IMAGE" \
              --image-pull-policy=Always \
              -- sleep 10
          done
          kubectl apply -n airflow -f cubes_job_config_map.yaml
          kubectl apply -n airflow -f ingest_job_config_map.yaml
          cd ../
          kubectl apply -n airflow -f airflow-read-configmap-role.yaml
          kubectl apply -n airflow -f spark-rbac-config.yaml
  trigger_dag:
    runs-on: ubuntu-latest
    needs: [ setup_infra, setup_spark_airflow_operator ]
    environment: cicd
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: v3.14.0

      - name: Install doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DO_TOKEN }}

      - name: üêçSet up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd infra-as-code
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Pulumi CLI
        uses: pulumi/actions@v4
        with:
          pulumi-version: latest

      - name: Login to Pulumi
        run: pulumi login
        env:
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}

      - name: üíæSave DigitalOcean kubeconfig
        run: |
          cd infra-as-code
          doctl kubernetes cluster kubeconfig save ${{ needs.setup_infra.outputs.k8s_cluster_id }}
      - name: üöÄ compute Dag Compute date
        run: |
          DAG_COMPUTE_DATE=$(date -d '2 days ago' '+%Y-%m-%d')
          echo "DAG_COMPUTE_DATE=$DAG_COMPUTE_DATE" >> $GITHUB_ENV
      - name:  üî´ trigger & Check airflow dag status
        run: |
          export SERVICE_NAME="airflow-lb"
          export NAMESPACE="airflow"
          if kubectl get svc "$SERVICE_NAME" -n "$NAMESPACE" >/dev/null 2>&1; then
            echo "Service $SERVICE_NAME already exists in namespace $NAMESPACE. Skipping expose."
          else
            echo "Service $SERVICE_NAME does not exist. Creating..."
            kubectl -n "$NAMESPACE" expose svc airflow-webserver \
              --type=LoadBalancer \
              --name="$SERVICE_NAME"
          fi
          #kubectl -n airflow expose svc airflow-webserver --type=LoadBalancer --name=airflow-lb
          touch airflow-lb-ip.txt
          EXTERNAL_IP=$(kubectl -n airflow get svc airflow-lb -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
          
          if [ -z "$EXTERNAL_IP" ]; then
            echo "LoadBalancer IP not assigned or service does not exist. lets wait"
            sleep 360
          fi          
          EXTERNAL_IP=$(kubectl -n airflow get svc airflow-lb -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
          #kubectl -n airflow get svc airflow-lb |grep -v NAME |awk '{print $4}' | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' > airflow-lb-ip.txt
          #cat airflow-lb-ip.txt
          #export AIRFLOW_LB_IP=$(cat airflow-lb-ip.txt)
          export AIRFLOW_LB_IP=$EXTERNAL_IP
          echo "AIRFLOW_LB_IP=$AIRFLOW_LB_IP" >> $GITHUB_ENV
          sleep 300
          cd scripts
          bash trigger_dag.sh ${{ env.DAG_NAME }} ${{ env.DAG_COMPUTE_DATE }} ${{ secrets.AF_USER }} ${{ secrets.AF_KEY }} $AIRFLOW_LB_IP
          sleep ${{ env.DAG_RUN_TIME_SEC }}
      - name: üü¢üî¥üü¢ Check airflow dag status
        run: |
          cd scripts
          RUN_ID=`cat /tmp/dag_run_id`
          echo "RUN_ID=$RUN_ID"
          bash check_dag_status.sh ${{ env.DAG_NAME }} $RUN_ID ${{ secrets.AF_USER }} ${{ secrets.AF_KEY }} ${{ env.AIRFLOW_LB_IP }}


  tear_down:
    runs-on: ubuntu-latest
    needs: [ setup_infra, trigger_dag ]
    environment: cicd
    if: always()
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: v3.14.0

      - name: Install doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DO_TOKEN }}

      - name: üêçSet up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd infra-as-code
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Pulumi CLI
        uses: pulumi/actions@v4
        with:
          pulumi-version: latest

      - name: Login to Pulumi
        run: pulumi login
        env:
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}

      - name: üíæSave DigitalOcean kubeconfig
        run: |
          cd infra-as-code
          doctl kubernetes cluster kubeconfig save ${{ needs.setup_infra.outputs.k8s_cluster_id }}
      - name: üí•tear down all resources
        run: |
          cd infra-as-code
          export PULUMI_STACK_NAME=${{ github.event.inputs.stack_name }}
          export TEAR_DOWN=${{ github.event.inputs.should_tear_down }}
          if [ "$TEAR_DOWN" != "true" ]; then
            echo "Skipping tear down as per input."
            exit 0
          fi
          if [ -z "$PULUMI_STACK_NAME" ]; then
            echo "PULUMI_STACK_NAME is not set, using default 'prod'"
            export PULUMI_STACK_NAME="prod"
          fi
          pulumi stack select $PULUMI_STACK_NAME
          pulumi config set digitalocean:token ${{ secrets.DO_TOKEN }} --secret
          pulumi refresh --yes
          echo "Deleting airflow load balancer service..."
          kubectl -n airflow delete svc airflow-lb --ignore-not-found
          helm uninstall spark-operator -n spark-operator 
          helm uninstall airflow -n airflow
          kubectl delete all --all  --wait=true --timeout=0s
          sleep 180
          kubectl delete pvc --all
          kubectl delete secrets --all
          kubectl delete configmaps --all
          echo "Deleting K8S cluster..."
          pulumi destroy --remove --yes
          echo "Deleting orphan pvc volumes in cloud..."
          echo "list of orphans...."
          doctl compute volume list --format ID,Name,DropletIDs | awk '$3 == "" && $2 ~ /^pvc-/'
          echo "Deleting orphan volumes..."
          doctl compute volume list --format ID,Name,DropletIDs --no-header | awk '$3 == "" && $2 ~ /^pvc-/ { print $1 }' | xargs -n1 -I {} doctl compute volume delete {} --force
          echo "Deleting other volumes..."
          sleep 300
          doctl compute volume list --format ID,Name,DropletIDs | awk '$3 != "[]" && $2 ~ /^pvc-/'
          doctl compute volume list --format ID,Name,DropletIDs --no-header | awk '$3 != "[]" && $2 ~ /^pvc-/ { print $1 }' | xargs -n1 -I {} doctl compute volume delete {} --force