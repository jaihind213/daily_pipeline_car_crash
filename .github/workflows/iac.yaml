name: Infra as Code on Digital Ocean

on:
  push:
    branches:
      - dev
  workflow_dispatch:

env:
  K8S_BOOTUP_TIME_SEC: 180
jobs:
  setup_infra:
    runs-on: ubuntu-latest
    environment: cicd
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: v3.14.0

      - name: 🦭Install kubeseal
        uses: digitalservicebund/setup-kubeseal@v1.0.0

      - name: Install envsubst
        run: sudo apt-get update && sudo apt-get install -y gettext

      - name: Install doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DO_TOKEN }}

      - name: 🐍Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd infra-as-code
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Pulumi CLI
        uses: pulumi/actions@v4
        with:
          pulumi-version: latest

      - name: Login to Pulumi
        run: pulumi login
        env:
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}

      - name: 🏗️Set up Pulumi stack
        run: |
          cd infra-as-code
          pulumi stack select prod || pulumi stack init prod
          pulumi config set digitalocean:token ${{ secrets.DO_TOKEN }} --secret
          pulumi refresh --yes
          OUTPUT=$(pulumi stack output k8s_cluster_id || echo "")
          if [ -n "$OUTPUT" ]; then
            echo "K8S_CLUSTER_ID=$OUTPUT" >> $GITHUB_ENV
            echo "✅ Exported K8S_CLUSTER_ID"
            export K8S_CLUSTER_ID=$OUTPUT
          fi
      - name: 🚚Deploy with Pulumi
        run: |
          cd infra-as-code
          pulumi up --yes
          echo "K8S_CLUSTER_ID=$(pulumi stack output k8s_cluster_id)" >> $GITHUB_ENV
      - name: 💾Save DigitalOcean kubeconfig
        run: |
          cd infra-as-code
          doctl kubernetes cluster kubeconfig save ${{ env.K8S_CLUSTER_ID }}
      - name: ⚠️ Check if k8s nodes ready
        run: |
          sleep ${{ env.K8S_BOOTUP_TIME_SEC }}
          export NOT_READY_NODES=$(kubectl get nodes --no-headers | awk '$2 != "Ready"')
          if [ -n "$NOT_READY_NODES" ]; then
            echo "$NOT_READY_NODES"
            echo "❌ Some nodes are still NotReady after waiting:"
            exit 1
          else
            echo "✅ All nodes are Ready."  
          fi
      - name: Add helm repos
        run: |
          helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets
          helm repo update
          helm repo add apache-airflow https://airflow.apache.org
          helm repo update
          #helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator # gives 404, was working.
          helm uninstall spark-operator -n spark-operator  || echo "it ok to fail"
          helm repo add --force-update spark-operator https://kubeflow.github.io/spark-operator
          helm repo update
          helm repo list
      - name: ✨Spark Setup
        run: |
          kubectl create namespace airflow || echo "airflow namespace exists"
          kubectl delete namespace spark-operator || echo "it is ok to fail"
          kubectl create namespace spark-operator || echo "spark-operator namespace exists"
          #git clone https://github.com/GoogleCloudPlatform/spark-on-k8s-operator.git
          #cd spark-on-k8s-operator/charts/spark-operator-chart
          #helm install spark-operator spark-operator/spark-operator  --set webhook.enable=true --set webhook.port=443 --namespace spark-operator --create-namespace
          #helm install spark-operator .  --set webhook.enable=true --set webhook.port=443 --namespace spark-operator --create-namespace
          #helm install spark-operator spark-operator/spark-operator --set sparkJobNamespace=*  --set webhook.enable=true --set webhook.enableCertManager=false --set webhook.servicePort=443 --set webhook.port=8443 --namespace spark-operator --create-namespace
          #helm install spark-operator spark-operator/spark-operator  --set serviceAccounts.spark.name=spark-operator-controller --set sparkjobNamespaces={airflow}  --set webhook.enable=true --set webhook.enableCertManager=false --set webhook.servicePort=443 --set webhook.port=8443 --namespace spark-operator
          helm install spark-operator spark-operator/spark-operator  --set serviceAccounts.spark.name=spark-operator-controller --set spark.jobNamespaces={airflow}  --set webhook.enable=true --set webhook.enableCertManager=false --set webhook.servicePort=443 --set webhook.port=8443 --namespace spark-operator --wait --timeout 500s
          sleep ${{ env.K8S_BOOTUP_TIME_SEC }}
          helm repo list
          cd infra-as-code/k8s
          kubectl apply -n airflow -f spark-rbac.yaml
          kubectl apply -n airflow -f spark-operator-cluster-rbac.yaml
          #check any errors
          kubectl logs -n spark-operator -l app.kubernetes.io/name=spark-operator |grep -vi err
      - name: 🌀༄ Airflow secret 🔐
        run: |
          kubectl create namespace sealed-secrets  || echo "sealed-secrets namespace exists"
          helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets || echo "sealed secrets repo added"
          helm repo update
          helm upgrade --install sealed-secrets-controller sealed-secrets/sealed-secrets --namespace sealed-secrets
          sleep 90
          kubectl get pods -A | grep sealed-secrets || echo "tried to get pods"
          kubectl create secret generic git-credentials --namespace airflow  --dry-run=client --from-literal=GIT_SYNC_USERNAME='${{ secrets.GIT_SYNC_USERNAME }}' --from-literal=GITSYNC_USERNAME='${{ secrets.GIT_SYNC_USERNAME }}' --from-literal=GIT_SYNC_PASSWORD='${{ secrets.GIT_SYNC_PASSWORD }}' --from-literal=GITSYNC_PASSWORD='${{ secrets.GIT_SYNC_PASSWORD }}' --from-literal=password=LOL -o json | kubeseal --namespace airflow --controller-namespace=sealed-secrets --scope=strict --format=yaml > my-secret-sealed.yaml
          kubectl apply -f my-secret-sealed.yaml -n airflow
          export SOCRATA_TOKEN=${{ secrets.SOCRATA_APP_TOKEN }}
          export S3_ACCESS_KEY=${{ secrets.S3_ACCESS_KEY }}
          export S3_SECRET_KEY=${{ secrets.S3_SECRET_KEY }}
          export ICEBERG_PATH=${{ secrets.ICEBERG_PATH }}
          export RAW_DATA_PATH=${{ secrets.RAW_DATA_PATH }}
          cd infra-as-code/k8s
          envsubst < dag_secrets_template.yaml | kubectl create  --namespace airflow -f - --dry-run=client -o json | kubeseal --namespace airflow --controller-namespace=sealed-secrets --scope=strict --format=yaml > dag-sealed-secret.yaml
          kubectl apply -f dag-sealed-secret.yaml -n airflow
      - name: 🌀༄ Airflow setup
        run: |
          cd infra-as-code/airflow
          export PGKEY=${{ secrets.PGKEY }}
          export AF_FN=${{ secrets.AF_FN }}
          export AF_LN=${{ secrets.AF_LN }}
          export AF_EMAIL=${{ secrets.AF_EMAIL }}
          export AF_USER=${{ secrets.AF_USER }}
          export AF_KEY=${{ secrets.AF_KEY }}
               
          envsubst < airflow_values_template.yaml > airflow_values.yaml
          helm repo add apache-airflow https://airflow.apache.org || echo "airflow repo added"
          helm repo update
          sleep 60
          helm upgrade --install airflow apache-airflow/airflow -n airflow --create-namespace -f airflow_values.yaml --debug --v=9
          sleep ${{ env.K8S_BOOTUP_TIME_SEC }}
      - name: 📣 Apply Configs
        run: |
          cd infra-as-code/k8s
          kubectl apply -n airflow -f app_configs/common_config_map.yaml
          kubectl apply -n airflow -f app_configs/cubes_job_config_map.yaml
          kubectl apply -n airflow -f app_configs/ingest_job_config_map.yaml
          kubectl apply -n airflow -f airflow-read-configmap-role.yaml
          kubectl apply -n airflow -f spark-rbac-config.yaml
      - name: 🚀 trigger airflow dag
        run: |
          sleep 10
#          kubectl -n airflow get svc |grep airflow-webserver
#          kubectl -n airflow expose svc airflow-webserver --type=LoadBalancer --name=airflow-lb
#          sleep 180
#          kubectl -n airflow get svc airflow-lb |grep -v NAME |awk '{print $4}' | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' > airflow-lb-ip.txt
#          export AIRFLOW_LB_IP=$(cat airflow-lb-ip.txt)
#          echo "AIRFLOW_LB_IP=$AIRFLOW_LB_IP" >> $GITHUB_ENV

      - name: 🔥Tear down
        if: always()
        run: |
          cd infra-as-code
          #pulumi stack select prod
          #pulumi destroy --remove --yes
