name: Infra as Code on Digital Ocean

on:
  push:
    branches:
      - dev
  workflow_dispatch:

env:
  K8S_BOOTUP_TIME_SEC: 180
  DAG_RUN_TIME_SEC: 300
jobs:
  setup_infra:
    runs-on: ubuntu-latest
    environment: cicd
    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: v3.14.0

      - name: ðŸ¦­Install kubeseal
        uses: digitalservicebund/setup-kubeseal@v1.0.0

      - name: Install envsubst
        run: sudo apt-get update && sudo apt-get install -y gettext

      - name: Install doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DO_TOKEN }}

      - name: ðŸSet up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd infra-as-code
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Install Pulumi CLI
        uses: pulumi/actions@v4
        with:
          pulumi-version: latest

      - name: Login to Pulumi
        run: pulumi login
        env:
          PULUMI_ACCESS_TOKEN: ${{ secrets.PULUMI_ACCESS_TOKEN }}

      - name: ðŸ—ï¸Set up Pulumi stack
        run: |
          cd infra-as-code
          pulumi stack select prod || pulumi stack init prod
          pulumi config set digitalocean:token ${{ secrets.DO_TOKEN }} --secret
          pulumi refresh --yes
          OUTPUT=$(pulumi stack output k8s_cluster_id || echo "")
          if [ -n "$OUTPUT" ]; then
            echo "K8S_CLUSTER_ID=$OUTPUT" >> $GITHUB_ENV
            echo "âœ… Exported K8S_CLUSTER_ID"
            export K8S_CLUSTER_ID=$OUTPUT
          fi
      - name: ðŸššDeploy with Pulumi
        run: |
          cd infra-as-code
          pulumi up --yes
          #pulumi stack select prod
          #pulumi destroy --remove --yes
          echo "K8S_CLUSTER_ID=$(pulumi stack output k8s_cluster_id)" >> $GITHUB_ENV
      - name: ðŸ’¾Save DigitalOcean kubeconfig
        run: |
          cd infra-as-code
          doctl kubernetes cluster kubeconfig save ${{ env.K8S_CLUSTER_ID }}
      - name: âš ï¸ Check if k8s nodes ready
        run: |
          export NOT_READY_NODES=$(kubectl get nodes --no-headers | awk '$2 != "Ready"')
          if [ -n "$NOT_READY_NODES" ]; then
            echo "âš ï¸ Some nodes are NotReady: lets wait"
            echo "$NOT_READY_NODES"
            sleep ${{ env.K8S_BOOTUP_TIME_SEC }}
            export NOT_READY_NODES=$(kubectl get nodes --no-headers | awk '$2 != "Ready"')
            if [ -n "$NOT_READY_NODES" ]; then
                echo "âŒ Some nodes are still NotReady after waiting:"
                echo "$NOT_READY_NODES"
                exit 1
            else
              echo "âœ… All nodes are Ready."
            fi
          else
            echo "âœ… All nodes are Ready."  
          fi
      - name: Add helm repos
        run: |
          helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets
          helm repo update
          helm repo add apache-airflow https://airflow.apache.org
          helm repo update
          #helm repo add spark-operator https://googlecloudplatform.github.io/spark-on-k8s-operator # gives 404, was working.
          helm uninstall spark-operator -n spark-operator  || echo "it ok to fail"
          helm uninstall airflow -n airflow  || echo "it ok to fail"
          helm repo add --force-update spark-operator https://kubeflow.github.io/spark-operator
          helm repo update
          helm repo list
      - name: âœ¨Spark Setup
        run: |
          kubectl create namespace airflow || echo "airflow namespace exists"
          kubectl delete namespace spark-operator || echo "it is ok to fail"
          kubectl create namespace spark-operator || echo "spark-operator namespace exists"
          #git clone https://github.com/GoogleCloudPlatform/spark-on-k8s-operator.git
          #cd spark-on-k8s-operator/charts/spark-operator-chart
          #helm install spark-operator spark-operator/spark-operator  --set webhook.enable=true --set webhook.port=443 --namespace spark-operator --create-namespace
          #helm install spark-operator .  --set webhook.enable=true --set webhook.port=443 --namespace spark-operator --create-namespace
          #helm install spark-operator spark-operator/spark-operator --set sparkJobNamespace=*  --set webhook.enable=true --set webhook.enableCertManager=false --set webhook.servicePort=443 --set webhook.port=8443 --namespace spark-operator --create-namespace
          #helm install spark-operator spark-operator/spark-operator  --set serviceAccounts.spark.name=spark-operator-controller --set sparkjobNamespaces={airflow}  --set webhook.enable=true --set webhook.enableCertManager=false --set webhook.servicePort=443 --set webhook.port=8443 --namespace spark-operator
          helm install spark-operator spark-operator/spark-operator  --set serviceAccounts.spark.name=spark-operator-controller --set spark.jobNamespaces={airflow}  --set webhook.enable=true --set webhook.enableCertManager=false --set webhook.servicePort=443 --set webhook.port=8443 --namespace spark-operator
          
          sleep 180
          helm repo list
          cd infra-as-code/k8s
          kubectl apply -n airflow -f spark-rbac.yaml
          kubectl apply -n airflow -f spark-operator-cluster-rbac.yaml
          #check any errors
          kubectl logs -n spark-operator -l app.kubernetes.io/name=spark-operator |grep -vi err
          #kubectl apply -f spark-operator-role-binding.yaml
          #kubectl apply -f spark-operator-role.yaml
      - name: ðŸŒ€à¼„ Airflow secret ðŸ”
        run: |
          kubectl create namespace sealed-secrets  || echo "sealed-secrets namespace exists"
          helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets || echo "sealed secrets repo added"
          helm repo update
          helm upgrade --install sealed-secrets-controller sealed-secrets/sealed-secrets --namespace sealed-secrets
          sleep 90
          kubectl get pods -A | grep sealed-secrets || echo "tried to get pods"
          kubectl create secret generic git-credentials --namespace airflow  --dry-run=client --from-literal=GIT_SYNC_USERNAME='${{ secrets.GIT_SYNC_USERNAME }}' --from-literal=GITSYNC_USERNAME='${{ secrets.GIT_SYNC_USERNAME }}' --from-literal=GIT_SYNC_PASSWORD='${{ secrets.GIT_SYNC_PASSWORD }}' --from-literal=GITSYNC_PASSWORD='${{ secrets.GIT_SYNC_PASSWORD }}' --from-literal=password=LOL -o json | kubeseal --namespace airflow --controller-namespace=sealed-secrets --scope=strict --format=yaml > my-secret-sealed.yaml
          kubectl apply -f my-secret-sealed.yaml -n airflow
          export SOCRATA_TOKEN=${{ secrets.SOCRATA_APP_TOKEN }}
          export S3_ACCESS_KEY=${{ secrets.S3_ACCESS_KEY }}
          export S3_SECRET_KEY=${{ secrets.S3_SECRET_KEY }}
          export ICEBERG_PATH=${{ secrets.ICEBERG_PATH }}
          export RAW_DATA_PATH=${{ secrets.RAW_DATA_PATH }}
          cd infra-as-code/k8s
          envsubst < dag_secrets_template.yaml | kubectl create  --namespace airflow -f - --dry-run=client -o json | kubeseal --namespace airflow --controller-namespace=sealed-secrets --scope=strict --format=yaml > dag-sealed-secret.yaml
          kubectl apply -f dag-sealed-secret.yaml -n airflow
      - name: ðŸŒ€à¼„ Airflow setup
        run: |
          cd infra-as-code/airflow
          export PGKEY=${{ secrets.PGKEY }}
          export AF_FN=${{ secrets.AF_FN }}
          export AF_LN=${{ secrets.AF_LN }}
          export AF_EMAIL=${{ secrets.AF_EMAIL }}
          export AF_USER=${{ secrets.AF_USER }}
          export AF_KEY=${{ secrets.AF_KEY }}
               
          envsubst < airflow_values_template.yaml > airflow_values.yaml
          helm repo add apache-airflow https://airflow.apache.org || echo "airflow repo added"
          helm repo update
          helm upgrade --install airflow apache-airflow/airflow -n airflow --create-namespace -f airflow_values.yaml
          sleep 180
      - name: ðŸ“£ Apply Configs
        run: |
          cd infra-as-code/k8s/app_configs
          kubectl apply -n airflow -f common_config_map.yaml
          kubectl apply -n airflow -f cubes_job_config_map.yaml
          kubectl apply -n airflow -f ingest_job_config_map.yaml
          cd ../
          kubectl apply -n airflow -f airflow-read-configmap-role.yaml
          kubectl apply -n airflow -f spark-rbac-config.yaml
      - name: ðŸš€ compute Dag Compute date
        run: |
          DAG_COMPUTE_DATE=$(date -d '2 days ago' '+%Y-%m-%d')
          echo "DAG_COMPUTE_DATE=$DAG_COMPUTE_DATE" >> $GITHUB_ENV
          sleep 180
#          kubectl -n airflow get svc |grep airflow-webserver
#          kubectl -n airflow expose svc airflow-webserver --type=LoadBalancer --name=airflow-lb
#          sleep 180
#          kubectl -n airflow get svc airflow-lb |grep -v NAME |awk '{print $4}' | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' > airflow-lb-ip.txt
#          export AIRFLOW_LB_IP=$(cat airflow-lb-ip.txt)
#          echo "AIRFLOW_LB_IP=$AIRFLOW_LB_IP" >> $GITHUB_ENV
      - name:  ðŸ”« trigger & Check airflow dag status
        run: |
          export SERVICE_NAME="airflow-lb"
          export NAMESPACE="airflow"
          if kubectl get svc "$SERVICE_NAME" -n "$NAMESPACE" >/dev/null 2>&1; then
            echo "Service $SERVICE_NAME already exists in namespace $NAMESPACE. Skipping expose."
          else
            echo "Service $SERVICE_NAME does not exist. Creating..."
            kubectl -n "$NAMESPACE" expose svc airflow-webserver \
              --type=LoadBalancer \
              --name="$SERVICE_NAME"
          fi
          #kubectl -n airflow expose svc airflow-webserver --type=LoadBalancer --name=airflow-lb
          kubectl -n airflow get svc airflow-lb |grep -v NAME |awk '{print $4}' | grep -Eo '([0-9]{1,3}\.){3}[0-9]{1,3}' > airflow-lb-ip.txt
          export AIRFLOW_LB_IP=$(cat airflow-lb-ip.txt)
          echo "AIRFLOW_LB_IP=$AIRFLOW_LB_IP" >> $GITHUB_ENV
          sleep 180
          bash trigger_dag.sh hello_world ${{ env.DAG_COMPUTE_DATE }} ${{ secrets.AF_USER }} ${{ secrets.AF_KEY }} $AIRFLOW_LB_IP
          sleep ${{ env.DAG_RUN_TIME_SEC }}
      - name: ðŸŸ¢ðŸ”´ðŸŸ¢ Check airflow dag status
        run: |
          RUN_ID=`cat /tmp/dag_run_id`
          echo "RUN_ID=$RUN_ID"
          bash check_dag_status.sh chicago_car_crash_pipeline $RUN_ID ${{ secrets.AF_USER }} ${{ secrets.AF_KEY }} ${{ env.AIRFLOW_LB_IP }}
      - name: ðŸ”¥Tear down
        if: always()
        run: |
          cd infra-as-code
          #pulumi stack select prod
          #pulumi destroy --remove --yes
